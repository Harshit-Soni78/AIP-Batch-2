{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (from requests) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (from requests) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (from requests) (2025.1.31)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\dell-os\\anaconda3\\envs\\demo\\lib\\site-packages (from beautifulsoup4) (4.12.2)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.3 soupsieve-2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~umpy (C:\\Users\\DELL-OS\\anaconda3\\envs\\demo\\Lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\DELL-\n",
      "[nltk_data]     OS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\DELL-\n",
      "[nltk_data]     OS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary of the Article:\n",
      " However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence,[396][397] in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision.Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI,[388] with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did \"not actually use AI in a material way\".The emergence of advanced generative AI in the midst of the AI boom and its ability to create and modify content exposed several unintended consequences and harms in the present and raised concerns about the risks of AI and its long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok).\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import heapq\n",
    "\n",
    "def get_text_from_url(url):\n",
    "    \"\"\"Fetches and extracts text from a given url\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    #extract text from paragraphs\n",
    "    paragraphs = soup.find_all(\"p\")\n",
    "    article_text = \"\".join([para.get_text() for para in paragraphs])\n",
    "\n",
    "    return article_text\n",
    "\n",
    "def summarize_text(text, num_sentences=3):\n",
    "    \"\"\"Summarize the given text using word frequency.\"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    #stopwords and word frequency\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_frequencies = {}\n",
    "\n",
    "    for word in word_tokenize(text.lower()):\n",
    "        if word.isalnum() and word not in stop_words:\n",
    "            word_frequencies[word] = word_frequencies.get(word,0)+1\n",
    "\n",
    "    max_freq = max(word_frequencies.values(), default=1)\n",
    "    for word in word_frequencies:\n",
    "        word_frequencies[word] /= max_freq\n",
    "\n",
    "    #sentence scoring\n",
    "    sentence_scores = {}\n",
    "    for sentence in sentences:\n",
    "        for word in word_tokenize(sentence.lower()):\n",
    "            if word in word_frequencies:\n",
    "                sentence_scores[sentence] = sentence_scores.get(sentence,0) + word_frequencies[word]\n",
    "\n",
    "    #get top ranked sentences\n",
    "    summary_sentences = heapq.nlargest(num_sentences, sentence_scores, key=sentence_scores.get)\n",
    "    return \"\".join(summary_sentences)\n",
    "\n",
    "\n",
    "#example- summarizing an online article\n",
    "\n",
    "url = \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n",
    "article_text = get_text_from_url(url)\n",
    "\n",
    "if article_text:\n",
    "    summary = summarize_text(article_text, num_sentences=5)\n",
    "    print(\"\\nSummary of the Article:\\n\", summary)\n",
    "else:\n",
    "    print(\"Could not extract text from the url.\")    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
