# Gesture Recognition for Blind Assistance ğŸ‘‹ğŸ”Š

## Overview ğŸŒ
This project implements a real-time gesture recognition system designed to assist visually impaired individuals by converting hand gestures into audible speech. The system uses computer vision and machine learning to detect hand gestures with two operational modes:
1. **ğŸ‘ï¸ğŸ—¨ï¸ Blind Assistance Mode**: Predefined gestures for common assistance requests
2. **ğŸ¤Ÿ ASL Mode**: American Sign Language alphabet recognition

## âœ¨ Key Features
- ğŸ–ï¸ Real-time hand gesture detection using MediaPipe
- ğŸ”Š Text-to-speech feedback for recognized gestures
- ğŸ¨ Simple UI with toggle buttons for mode switching
- â³ Cooldown period to prevent gesture spamming
- ğŸ‘€ Visual feedback of recognized gestures
- ğŸ’¡ Future-ready for wearable integration

## ğŸ¯ Target Audience
This system is designed for:
- ğŸ‘©ğŸ¦¯ Visually impaired individuals
- ğŸ¤Ÿ ASL users
- ğŸ¥ Healthcare and accessibility applications

## ğŸ“‹ Requirements
- Python 3.6+ ğŸ
- OpenCV (`pip install opencv-python`) ğŸ“·
- MediaPipe (`pip install mediapipe`) âœ‹
- pyttsx3 (`pip install pyttsx3`) ğŸ”Š

## ğŸ¤² Gesture Definitions

### ğŸ‘ï¸ğŸ—¨ï¸ Blind Assistance Mode Gestures
| Gesture   | Finger State | Message                      | Emoji |
|-----------|--------------|------------------------------|-------|
| EMERGENCY | [0,0,0,0,0]  | "Emergency! Need help!"       | ğŸ†˜    |
| REPEAT    | [0,1,0,0,0]  | "Please repeat that"          | ğŸ”    |
| HELP      | [0,1,1,0,0]  | "I need assistance"           | ğŸ™‹    |
| LOCATION  | [1,0,0,0,1]  | "Where am I?"                | ğŸ—ºï¸    |
| GUIDE     | [1,1,0,0,0]  | "Please guide me"            | ğŸš¶â€â™‚ï¸   |

### ğŸ¤Ÿ ASL Mode
Recognizes letters A-Z (excluding J) from American Sign Language alphabet with visual feedback ğŸ‘†ğŸ‘‡ğŸ–ï¸

## ğŸš€ Usage Guide
1. Run the script: `python gesture_assistant.py` ğŸ’»
2. The system will open a camera window with:
   - ğŸ”˜ Left button: Toggle between Blind/ASL modes
   - ğŸ”˜ Right button: Start/Stop recognition
3. Make gestures to trigger voice feedback ğŸ—£ï¸

## âš™ï¸ Technical Details
- Uses MediaPipe's hand landmark model âœ‹
- Calculates joint angles for precise detection ğŸ“
- 1.5s cooldown between detections â±ï¸
- Visual feedback overlay ğŸ‘€

## ğŸ”® Future Roadmap
- **Wearable Integration** âŒš
  - Smart wristbands for hands-free interaction
  - Haptic feedback for tactile response ğŸ“³
  - IoT connectivity for remote monitoring ğŸŒ
  
- **Enhanced Features** ğŸš€
  - Multilingual speech synthesis ğŸŒ
  - Deep learning gesture recognition ğŸ§ 
  - Mobile app companion ğŸ“±
  - Gesture sequences for complex commands ğŸ”¢

- **Accessibility Improvements** â™¿
  - Voice command integration ğŸ¤
  - Environmental awareness features ğŸ™ï¸
  - Emergency alert system ğŸš¨

## ğŸ“œ License
MIT License - Open Source âœ…

## ğŸ™ Acknowledgments
- MediaPipe for hand tracking âœ‹
- pyttsx3 for TTS ğŸ”Š
- OpenCV for vision capabilities ğŸ‘ï¸
- All contributors and testers ğŸ¤

---

ğŸ’¡ **Prototype Note**: Current version focuses on camera-based recognition, with wearable integration planned for Phase 2 development. We welcome collaborators to help make this technology more accessible! ğŸŒˆ
