{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in c:\\python312\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: keras in c:\\python312\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (2.19.0rc0)\n",
      "Requirement already satisfied: nltk in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (3.9.1)\n",
      "Requirement already satisfied: absl-py in c:\\python312\\lib\\site-packages (from keras) (2.1.0)\n",
      "Requirement already satisfied: rich in c:\\python312\\lib\\site-packages (from keras) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\python312\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: h5py in c:\\python312\\lib\\site-packages (from keras) (3.12.1)\n",
      "Requirement already satisfied: optree in c:\\python312\\lib\\site-packages (from keras) (0.13.1)\n",
      "Requirement already satisfied: ml-dtypes in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from keras) (0.5.1)\n",
      "Requirement already satisfied: packaging in c:\\python312\\lib\\site-packages (from keras) (24.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\python312\\lib\\site-packages (from tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\python312\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\python312\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\python312\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\python312\\lib\\site-packages (from tensorflow) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\python312\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\python312\\lib\\site-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (1.71.0rc2)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: click in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\python312\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2024.8.30)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\python312\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\harsh\\appdata\\roaming\\python\\python312\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: colorama in c:\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\python312\\lib\\site-packages (from rich->keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\python312\\lib\\site-packages (from rich->keras) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: Loading egg at c:\\python312\\lib\\site-packages\\vboxapi-1.0-py3.12.egg is deprecated. pip 25.1 will enforce this behaviour change. A possible replacement is to use pip for package installation. Discussion can be found at https://github.com/pypa/pip/issues/12330\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy keras tensorflow nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `nltk` – Used for tokenization and lemmatization.\n",
    "-   `json` – Loads the intent dataset from a JSON file.\n",
    "-   `pickle` – Saves and loads processed data (`words.pkl`, `classes.pkl`).\n",
    "-   `random` – Shuffles training data to improve generalization.\n",
    "-   `numpy` – Handles data processing and numerical operations.\n",
    "-   `WordNetLemmatizer` – Converts words to their base form (e.g., \"running\" → \"run\").\n",
    "-   `keras.models.Sequential` – Defines the neural network architecture.\n",
    "-   `keras.layers.Dense, Dropout` – Adds fully connected layers and dropout for regularization.\n",
    "-   `keras.optimizers.SGD` – Uses Stochastic Gradient Descent for training.\n",
    "-   `os` – Handles file operations.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensuring NLTK Resources are Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to check and download NLTK resources if not already available\n",
    "def check_nltk_resources():\n",
    "    try:\n",
    "        nltk.data.find('tokenizers/punkt')\n",
    "    except LookupError:\n",
    "        nltk.download('punkt')\n",
    "    \n",
    "    try:\n",
    "        nltk.data.find('corpora/wordnet')\n",
    "    except LookupError:\n",
    "        nltk.download('wordnet')\n",
    "\n",
    "# Call the function to ensure NLTK resources are available\n",
    "check_nltk_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   This function ensures that `punkt` (for tokenization) and `wordnet` (for lemmatization) are downloaded.\n",
    "-   If they are missing, it downloads them.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load and Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load and process the data\n",
    "data_file = open('Data/admission_data.json').read()\n",
    "intents = json.loads(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `WordNetLemmatizer()` – Initializes the lemmatizer for text preprocessing.\n",
    "-   `open('Data/admission_data.json').read()` – Opens and reads the JSON dataset.\n",
    "-   `json.loads(data_file)` – Converts the JSON string into a Python dictionary.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparing Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `words` – Stores unique words from all training sentences.\n",
    "-   `classes` – Stores different intent tags.\n",
    "-   `documents` – Stores word patterns mapped to intent tags.\n",
    "-   `ignore_words` – Excludes punctuation marks.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(intents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # Tokenize each word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)  # Tokenize each sentence\n",
    "        # Add to documents\n",
    "        words.extend(w)  # Add words to list\n",
    "        \n",
    "        documents.append((w, intent['tag']))  # Store word-intent pair\n",
    "        # Add to classes if not already present\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Loops through each intent in the dataset.\n",
    "-   Tokenizes each pattern sentence into words.\n",
    "-   Stores word-tag pairs in `documents` for training.\n",
    "-   Adds new intent tags to `classes`.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize and lower each word and remove duplicates\n",
    "words = sorted(list(set([lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_words])))\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(list(set(classes)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Converts all words to lowercase and lemmatizes them.\n",
    "-   Removes duplicates and sorts them.\n",
    "-   Sorts `classes` to maintain consistency.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data information\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Displays summary information about training data.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save words and classes to disk\n",
    "pickle.dump(words, open('Model/words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('Model/classes.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saves processed `words` and `classes` for later use.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Creating Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `training` – Stores input-output training data.\n",
    "-   `output_empty` – Initializes a list of zeros to represent class labels.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    # Initialize bag of words\n",
    "    bag = []\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [lemmatizer.lemmatize(word.lower()) for word in doc[0]]\n",
    "    # Create bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "        # Fills `bag` with `1` if the word appears in the document, else `0`.\n",
    "        \n",
    "    # Create output array\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    \n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Creates an empty bag-of-words representation.\n",
    "- Lemmatizes words in the current document.\n",
    "- Fills `bag` with `1` if the word appears in the document, else `0`.\n",
    "- Creates a one-hot encoded output array for the intent tag.\n",
    "- Appends the `bag` (input) and `output_row` (label) to `training`.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the data and convert to numpy arrays\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Randomly shuffles data to prevent bias.\n",
    "-   Converts `training` list into a NumPy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and test lists\n",
    "train_x = np.array(list(training[:, 0]))\n",
    "train_y = np.array(list(training[:, 1]))\n",
    "\n",
    "print(\"Training data created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Splits `training` data into `train_x` (features) and `train_y` (labels).\n",
    "- Confirms successful data preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Creating and Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(train_y[0]), activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   `Sequential()` – Defines a feed-forward neural network.\n",
    "-   `Dense(128, input_shape=(len(train_x[0]),), activation='relu')` – Adds a fully connected layer with 128 neurons and ReLU activation.\n",
    "-   `Dropout(0.5)` – Prevents overfitting by randomly deactivating neurons.\n",
    "-   `Dense(64, activation='relu')` – Adds another hidden layer with 64 neurons.\n",
    "-   `Dense(len(train_y[0]), activation='softmax')` – Outputs probability distribution over intent classes.\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Uses **Stochastic Gradient Descent (SGD)** as the optimizer.\n",
    "-   `categorical_crossentropy` is the loss function (used for multi-class classification).\n",
    "- `momentum=0.9`: Adds a fraction of the previous update to the current update to accelerate the gradient vectors in the right directions, thus leading to faster converging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and save the model\n",
    "hist = model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Trains the model for **200 epochs** using mini-batches of size **5**.\n",
    "-   `verbose=1` prints training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Model/chatbot_model.h5', hist)\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Saves the trained model as `chatbot_model.h5`.\n",
    "- Confirms the successful creation of the model.\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary of Key Steps**\n",
    "\n",
    "1.  **Load and preprocess dataset** – Tokenization, lemmatization, and intent tagging.\n",
    "2.  **Create bag-of-words representations** – Convert text data into numerical format.\n",
    "3.  **Train a Neural Network** – Using dense layers and dropout to classify intent.\n",
    "4.  **Save model and preprocessing files** – To be used in chatbot inference.\n",
    "\n",
    "This is the complete breakdown of `train_model.py`. Let me know if you want to go deeper into any part! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
